{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from pydotplus import graph_from_dot_data\n",
    "from IPython.display import Image\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score,f1_score,fbeta_score,mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score,f1_score,fbeta_score,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'E:\\Project\\1-Output\\1 AFTER FEATURE EXTRACTION DATASETS\\Feature_Extracted_Dataset-3.csv',header=0)\n",
    "\n",
    "df = df.drop('Domain Name',axis=1)\n",
    "df = df.drop('Sr. No.',axis=1)\n",
    "columns_to_encode = ['Https in URL','Is domain suspicious','Is image only in webpage',\n",
    "    'Is IP as Host name', 'Is .exe present', 'FTP used', 'Is www present',\n",
    "    '.js used', 'Files in URL', 'css used', 'Is Hashed', 'TLD',\n",
    "    'File Extention', 'Hyphenstring', 'Homoglyph', 'Vowel string',\n",
    "    'Bitsquatting', 'Insertion string', 'Omission', 'Repeatition',\n",
    "    'Replacement', 'Subdomain', 'Transposition', 'Addition string',\n",
    "    'TLD in Subdomain', 'TLD in path', 'https in host name',\n",
    "    'Word based distribution', 'Is English word', 'Is Meaningful',\n",
    "    'Is Pronounceable', 'Is random', 'IP Address', 'ASN Number',\n",
    "    'ASN Country Code', 'ASN CIDR', 'ASN Postal Code', 'ASN creation date',\n",
    "    'ASN updation date', 'Fake link in status bar', 'Right click disable',\n",
    "    'Popup window', 'mailto: present', 'Frame tag present', 'Is title tag empty'\n",
    "]\n",
    "df = pd.get_dummies(df, columns=columns_to_encode, drop_first=True)\n",
    "X = df.loc[:,df.columns!=\"Label\"]\n",
    "y = df[\"Label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "params_grid = {\n",
    "               'max_depth': [5,8,10,15,20,25,30,50],\n",
    "               'max_features': ['log2','sqrt',0.25,0.5,0.6,1.0],\n",
    "               'min_samples_leaf': [1,25,50,70],\n",
    "               'min_samples_split': [2,5,10,20]\n",
    "              }\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "grid_search = GridSearchCV(RandomForestClassifier (random_state = 42), param_grid = params_grid, n_jobs = -1, cv = cv, scoring = 'accuracy')\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: %s\" % (grid_result.best_params_))\n",
    "\n",
    "gbc_clf2 = RandomForestClassifier(max_depth = grid_result.best_params_.get('max_depth'),\n",
    "                     max_features = grid_result.best_params_.get('max_features'),\n",
    "                     min_samples_leaf = grid_result.best_params_.get('min_samples_leaf'),\n",
    "                     min_samples_split = grid_result.best_params_.get('min_samples_split'),\n",
    "                     random_state=42)\n",
    "\n",
    "gbc_clf2.fit(X_train, y_train)\n",
    "\n",
    "e=time.time()\n",
    "exe_time = round(e - s)\n",
    "print(\"Execution Time in Seconds :\", exe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'max_depth': 5, 'max_features': 7, 'min_samples_leaf': 4, 'min_samples_split': 5, 'n_estimators': 50}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORMANCE EVALUATION OF RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbc_clf2.predict(X_train)\n",
    "y_test_pred = gbc_clf2.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "print(\"-------------------------------------Metrics------------------------------------------\")\n",
    "print(\"Test accuracy score {:.4f}\".format(accuracy_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Recall {:.4f}\".format(recall_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Precision {:.4f}\".format(precision_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F1 Score {:.4f}\".format(f1_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F2 Score {:.4f}\".format(fbeta_score(y_test, y_test_pred, beta=2.0) * 100))\n",
    "\n",
    "print(\"--------------------------TPR, TNR, FPR, FNR------------------------------------------\")\n",
    "TPR = tp / (tp + fn)\n",
    "TNR = tn / (tn + fp)\n",
    "FPR = fp / (fp + tn)\n",
    "FNR = fn / (fn + tp)\n",
    "print(\"TPR {:.4f}\".format(TPR))\n",
    "print(\"TNR {:.4f}\".format(TNR))\n",
    "print(\"FPR {:.4f}\".format(FPR))\n",
    "print(\"FNR {:.4f}\".format(FNR))\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m cv \u001b[38;5;241m=\u001b[39m RepeatedStratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, n_repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     14\u001b[0m RD_search \u001b[38;5;241m=\u001b[39m GridSearchCV(AdaBoostClassifier(estimator\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(), random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),parameters,cv\u001b[38;5;241m=\u001b[39mcv,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m RD_result \u001b[38;5;241m=\u001b[39m \u001b[43mRD_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (RD_result\u001b[38;5;241m.\u001b[39mbest_params_))\n\u001b[0;32m     18\u001b[0m gbc_clf2 \u001b[38;5;241m=\u001b[39m AdaBoostClassifier(estimator\u001b[38;5;241m=\u001b[39mDecisionTreeClassifier(max_depth\u001b[38;5;241m=\u001b[39mRD_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator__max_depth\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[0;32m     19\u001b[0m                               max_features\u001b[38;5;241m=\u001b[39mRD_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mestimator__max_features\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[0;32m     20\u001b[0m                               learning_rate\u001b[38;5;241m=\u001b[39mRD_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     21\u001b[0m                               n_estimators\u001b[38;5;241m=\u001b[39mRD_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     22\u001b[0m                               algorithm\u001b[38;5;241m=\u001b[39mRD_result\u001b[38;5;241m.\u001b[39mbest_params_[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     23\u001b[0m                               random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "s =time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "parameters = {\n",
    "              'n_estimators': [10,50,100],\n",
    "              'learning_rate': [0.01,0.05,0.1,0.5],\n",
    "              'algorithm': ['SAMME','SAMME.R'],\n",
    "              'estimator__max_depth': [1,50,100,200,300,400,500],\n",
    "              'estimator__max_features': [2,10,100,200,300,400,500] \n",
    "             }\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "RD_search = GridSearchCV(AdaBoostClassifier(estimator=DecisionTreeClassifier(), random_state=42),parameters,cv=cv,n_jobs=-1,scoring='accuracy')\n",
    "RD_result = RD_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: %s\" % (RD_result.best_params_))\n",
    "\n",
    "gbc_clf2 = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=RD_result.best_params_['estimator__max_depth'], \n",
    "                              max_features=RD_result.best_params_['estimator__max_features']),\n",
    "                              learning_rate=RD_result.best_params_['learning_rate'],\n",
    "                              n_estimators=RD_result.best_params_['n_estimators'],\n",
    "                              algorithm=RD_result.best_params_['algorithm'],\n",
    "                              random_state=42)\n",
    "\n",
    "gbc_clf2.fit(X_train, y_train)\n",
    "\n",
    "e=time.time()\n",
    "exe_time = round(e - s)\n",
    "print(\"Execution Time in Seconds :\", exe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'algorithm': 'SAMME', 'estimator__max_depth': 10, 'estimator__max_features': 30, 'learning_rate': 0.01, 'n_estimators': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORMANCE EVALUATION OF ADABOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbc_clf2.predict(X_train)\n",
    "y_test_pred = gbc_clf2.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "print(\"-------------------------------------Metrics------------------------------------------\")\n",
    "print(\"Test accuracy score {:.4f}\".format(accuracy_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Recall {:.4f}\".format(recall_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Precision {:.4f}\".format(precision_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F1 Score {:.4f}\".format(f1_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F2 Score {:.4f}\".format(fbeta_score(y_test, y_test_pred, beta=2.0) * 100))\n",
    "\n",
    "print(\"--------------------------TPR, TNR, FPR, FNR------------------------------------------\")\n",
    "TPR = tp / (tp + fn)\n",
    "TNR = tn / (tn + fp)\n",
    "FPR = fp / (fp + tn)\n",
    "FNR = fn / (fn + tp)\n",
    "print(\"TPR {:.4f}\".format(TPR))\n",
    "print(\"TNR {:.4f}\".format(TNR))\n",
    "print(\"FPR {:.4f}\".format(FPR))\n",
    "print(\"FNR {:.4f}\".format(FNR))\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRADIENTBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "parameters = {\n",
    "              'loss': ['exponential'],\n",
    "              'learning_rate': [0.01,0.05,0.1,0.5],\n",
    "              'min_samples_split': [2,5,7,9],\n",
    "              'min_samples_leaf': [3,5,6,8],\n",
    "              'max_depth': [3,5,10,15],\n",
    "              'max_features': [3,5,10,15],\n",
    "              'criterion': ['friedman_mse'],\n",
    "              'subsample': [0.5,0.8,0.9,1.0],\n",
    "              'n_estimators': [10,50,100]\n",
    "             }\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "RD_search = GridSearchCV(GradientBoostingClassifier(random_state=42), parameters, cv=cv, n_jobs=-1, scoring='accuracy')\n",
    "RD_result = RD_search.fit(X_train, y_train)\n",
    "print(\"Best parameters: %s\" % (RD_result.best_params_))\n",
    "\n",
    "gbc_clf2 = GradientBoostingClassifier(learning_rate=RD_result.best_params_.get('learning_rate'),\n",
    "                                      loss=RD_result.best_params_.get('loss'),\n",
    "                                      min_samples_split=RD_result.best_params_.get('min_samples_split'),\n",
    "                                      min_samples_leaf=RD_result.best_params_.get('min_samples_leaf'),\n",
    "                                      max_depth=RD_result.best_params_.get('max_depth'),\n",
    "                                      max_features=RD_result.best_params_.get('max_features'),\n",
    "                                      criterion=RD_result.best_params_.get('criterion'),\n",
    "                                      subsample=RD_result.best_params_.get('subsample'),\n",
    "                                      n_estimators=RD_result.best_params_.get('n_estimators'),\n",
    "                                      random_state=42)\n",
    "\n",
    "gbc_clf2.fit(X_train, y_train)\n",
    "\n",
    "e=time.time()\n",
    "exe_time = round(e - s)\n",
    "print(\"Execution Time in Seconds :\", exe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'criterion': 'friedman_mse', 'learning_rate': 0.05, 'loss': 'exponential', 'max_depth': 10, 'max_features': 'log2', 'min_samples_leaf': 5, 'min_samples_split': 2, 'n_estimators': 50, 'subsample': 0.7}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORMANCE EVALUATION OF GRADIENTBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbc_clf2.predict(X_train)\n",
    "y_test_pred = gbc_clf2.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "print(\"-------------------------------------Metrics------------------------------------------\")\n",
    "print(\"Test accuracy score {:.4f}\".format(accuracy_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Recall {:.4f}\".format(recall_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Precision {:.4f}\".format(precision_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F1 Score {:.4f}\".format(f1_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F2 Score {:.4f}\".format(fbeta_score(y_test, y_test_pred, beta=2.0) * 100))\n",
    "\n",
    "print(\"--------------------------TPR, TNR, FPR, FNR------------------------------------------\")\n",
    "TPR = tp / (tp + fn)\n",
    "TNR = tn / (tn + fp)\n",
    "FPR = fp / (fp + tn)\n",
    "FNR = fn / (fn + tp)\n",
    "print(\"TPR {:.4f}\".format(TPR))\n",
    "print(\"TNR {:.4f}\".format(TNR))\n",
    "print(\"FPR {:.4f}\".format(FPR))\n",
    "print(\"FNR {:.4f}\".format(FNR))\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s =time.time()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "parameters = {\n",
    "              'objective': ['binary:logistic'],\n",
    "              'learning_rate': [0.05,0.1,0.5,1.0], \n",
    "              'max_depth': [2,4,6,8],\n",
    "              'min_child_weight': [3,5,7,9],\n",
    "              'subsample':  [0.1,0.3,0.5,0.7],\n",
    "              'colsample_bytree': [0.3,0.5,0.8],\n",
    "              'n_estimators': [10,50,100],\n",
    "              'gamma': [0.1,0.3,0.4,0.7]\n",
    "              }\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n",
    "RD = GridSearchCV(xgb.XGBClassifier(random_state=42), parameters, n_jobs=1, cv=cv, scoring='accuracy',verbose=0, refit=True)\n",
    "RD_result = RD.fit(X_train, y_train)\n",
    "print(\"Best parameters: %s\" % (RD.best_params_))\n",
    "\n",
    "gbc_clf2 = xgb.XGBClassifier(objective = RD.best_params_.get('objective'),\n",
    "                     learning_rate = RD.best_params_.get('learning_rate'),\n",
    "                     max_depth = RD.best_params_.get('max_depth'),\n",
    "                     min_child_weight = RD.best_params_.get('min_child_weight'),\n",
    "                     subsample = RD.best_params_.get('subsample'),\n",
    "                     colsample_bytree = RD.best_params_.get('colsample_bytree'),\n",
    "                     n_estimators = RD.best_params_.get('n_estimators'),\n",
    "                     gamma = RD.best_params_.get('gamma'),\n",
    "                     random_state=42)\n",
    "\n",
    "gbc_clf2.fit(X_train, y_train)\n",
    "\n",
    "e=time.time()\n",
    "exe_time = round(e - s)\n",
    "print(\"Execution Time in Seconds :\", exe_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters: {'colsample_bytree': 0.2, 'gamma': 0.8, 'learning_rate': 0.6, 'max_depth': 5, 'min_child_weight': 4, 'n_estimators': 50, 'objective': 'binary:logistic', 'subsample': 0.7}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERFORMANCE EVALUATION OF XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = gbc_clf2.predict(X_train)\n",
    "y_test_pred = gbc_clf2.predict(X_test)\n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_test_pred).ravel()\n",
    "\n",
    "print(\"-------------------------------------Metrics------------------------------------------\")\n",
    "print(\"Test accuracy score {:.4f}\".format(accuracy_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Recall {:.4f}\".format(recall_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test Precision {:.4f}\".format(precision_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F1 Score {:.4f}\".format(f1_score(y_test, y_test_pred) * 100))\n",
    "print(\"Test F2 Score {:.4f}\".format(fbeta_score(y_test, y_test_pred, beta=2.0) * 100))\n",
    "\n",
    "print(\"--------------------------TPR, TNR, FPR, FNR------------------------------------------\")\n",
    "TPR = tp / (tp + fn)\n",
    "TNR = tn / (tn + fp)\n",
    "FPR = fp / (fp + tn)\n",
    "FNR = fn / (fn + tp)\n",
    "print(\"TPR {:.4f}\".format(TPR))\n",
    "print(\"TNR {:.4f}\".format(TNR))\n",
    "print(\"FPR {:.4f}\".format(FPR))\n",
    "print(\"FNR {:.4f}\".format(FNR))\n",
    "print(confusion_matrix(y_test,y_test_pred))\n",
    "report = classification_report(y_test, y_test_pred)\n",
    "print(report)\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
